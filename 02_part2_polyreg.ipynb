{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db9c92c",
   "metadata": {},
   "source": [
    "## Part 2 — Polynomial regression and interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac4839",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Capture nonlinear and interaction effects using polynomial feature engineering and an explicit bias term:\n",
    "$$\n",
    "\\hat{L} = X * w + b.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50115d",
   "metadata": {},
   "source": [
    "# Importing libraries \n",
    "For this part, first of all, you have to import the libraries that we are going to use in this notebook. We are going to use the following libraries:\n",
    "\n",
    "- **pandas**: We are going to use this library for data manipulation and analysis.\n",
    "- **numpy**: We are going to use this library for numerical computations.\n",
    "- **matplotlib**: We are going to use this library for data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658cb027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run this once if needed)\n",
    "%pip install numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d909781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe6475",
   "metadata": {},
   "source": [
    "## Defining the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fe3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4])\n",
    "T = np.array([3800, 4400, 5800, 6400, 6900, 7400, 7900, 8300, 8800, 9200])\n",
    "L = np.array([0.15, 0.35, 1.00, 2.30, 4.10, 7.00, 11.2, 17.5, 25.0, 35.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b1216",
   "metadata": {},
   "source": [
    "## 1. Dataset Visualization\n",
    "\n",
    "The dataset contains observations of stellar properties: mass (M), temperature (T), and luminosity (L). Here is how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1feb286",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sc = plt.scatter(M, L, c=T, cmap='plasma', s=80)\n",
    "plt.colorbar(sc, label=\"Temperature (K)\")\n",
    "plt.xlabel(\"Stellar Mass (M)\")\n",
    "plt.ylabel(\"Luminosity (L)\")\n",
    "plt.title(\"Luminosity vs Mass (color = Temperature)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6482dfb",
   "metadata": {},
   "source": [
    "## 2. Feature engineering\n",
    "\n",
    "We create polynomial and interaction features to capture nonlinear relationships:\n",
    "\n",
    "$$X = [M, T, M^2, M \\cdot T]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb38b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(X):\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    X_norm = (X - mu) / sigma\n",
    "    return X_norm, mu, sigma\n",
    "\n",
    "X_full = np.column_stack((M, T, M**2, M*T))\n",
    "y = L\n",
    "\n",
    "print(\"First lines of X_full:\")\n",
    "print(X_full[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e8294",
   "metadata": {},
   "source": [
    "## 3. Model and Loss model\n",
    "The prediction model is this one:\n",
    "$$\n",
    "\\hat{L} = Xw + b\n",
    "$$\n",
    "\n",
    "and the loss model is the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac01d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    return X @ w + b\n",
    "\n",
    "def mse(y, y_hat):\n",
    "    return np.mean((y - y_hat) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4788c22c",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent Optimization\n",
    "\n",
    "To train our model, we use gradient descent to minimize the MSE loss function. The optimization process involves:\n",
    "\n",
    "- **Cost function**: Computes the mean squared error between predictions and actual values\n",
    "- **Gradients**: Calculate partial derivatives with respect to weights (w) and bias (b)\n",
    "- **Update rule**: Adjust parameters in the direction that reduces loss by step size α\n",
    "\n",
    "This iterative approach finds optimal weights and bias that best fit the nonlinear stellar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52abfdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    m = len(y)\n",
    "    errors = predict(X, w, b) - y\n",
    "    return (1/(2*m)) * np.sum(errors**2)\n",
    "\n",
    "def compute_gradients(X, y, w, b):\n",
    "    m = len(y)\n",
    "    errors = predict(X, w, b) - y\n",
    "    dw = (1/m) * (X.T @ errors)\n",
    "    db = (1/m) * np.sum(errors)\n",
    "    return dw, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b817b4",
   "metadata": {},
   "source": [
    "Here is a general formula for the polynomial regression model with interaction and bias term used in this notebook:\n",
    "\n",
    "$$\n",
    "\\hat{L} = w_1 M + w_2 T + w_3 M^2 + w_4 (M \\cdot T) + b\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e83968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w, b, alpha, epochs):\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        dw, db = compute_gradients(X, y, w, b)\n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "        losses.append(compute_cost(X, y, w, b))\n",
    "\n",
    "    return w, b, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e0f315",
   "metadata": {},
   "source": [
    "## 5. Feature selection experiment\n",
    "\n",
    "In this example, we will compare different sets of input features and how polynomial terms and interactions influence the performance of the models. This will help us understand the effect on the accuracy of the models and the importance of the features for modeling the luminosity of the stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9986924",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"M1 [M, T]\": np.column_stack((M, T)),\n",
    "    \"M2 [M, T, M^2]\": np.column_stack((M, T, M**2)),\n",
    "    \"M3 [M, T, M^2, M*T]\": np.column_stack((M, T, M**2, M*T))\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "alpha = 0.01    \n",
    "epochs = 5000\n",
    "\n",
    "for name, X_model in models.items():\n",
    "    X_norm, mu, sigma = normalize_features(X_model)\n",
    "\n",
    "    w = np.zeros(X_norm.shape[1])\n",
    "    b = 0.0\n",
    "\n",
    "    w_final, b_final, losses = gradient_descent(X_norm, y, w, b, alpha, epochs)\n",
    "    final_loss = losses[-1]\n",
    "\n",
    "    results[name] = (w_final, b_final, final_loss, mu, sigma)\n",
    "\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"w =\", w_final)\n",
    "    print(\"b =\", b_final)\n",
    "    print(\"Final Loss =\", final_loss)\n",
    "\n",
    "    preds = predict(X_norm, w_final, b_final)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(y, preds)\n",
    "    plt.plot([min(y), max(y)], [min(y), max(y)])\n",
    "    plt.xlabel(\"Actual Luminosity\")\n",
    "    plt.ylabel(\"Predicted Luminosity\")\n",
    "    plt.title(name)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b8ad5",
   "metadata": {},
   "source": [
    "## 6. Gradient Descent and Convergence\n",
    "We will train the polynomial regression model using the gradient descent optimization algorithm. We will specifically train the most complex model, M3, which includes all the features: mass (M), temperature (T), mass squared (M²), and the interaction term (M·T).\n",
    "\n",
    "**How Gradient Descent Works:**\n",
    "- It starts by initializing the parameters with random values\n",
    "- It calculates the rate at which the loss function changes with respect to the parameters\n",
    "- It adjusts the parameters to minimize the loss\n",
    "- This process is repeated until the algorithm converges\n",
    "\n",
    "In the following cells, we will present the training convergence graph, the effect of the interaction coefficient on the model, and the predictions made by the algorithm on the stellar observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43560e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = models[\"M3 [M, T, M^2, M*T]\"]\n",
    "X_norm, mu, sigma = normalize_features(X)\n",
    "\n",
    "w = np.zeros(X_norm.shape[1])\n",
    "b = 0\n",
    "\n",
    "w_trained, b_trained, losses = gradient_descent(X_norm, y, w, b, alpha, epochs)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Convergence (M3)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69676f48",
   "metadata": {},
   "source": [
    "## 7. Cost vs Interaction Coefficient\n",
    "\n",
    "In this section, the impact of the interaction weight $w_{M \\cdot T}$ on the cost of the model is discussed. The interaction weight is the weight of the interaction term $M \\cdot T$. The impact of the interaction weight on the cost of the model can be visualized by varying the interaction weight from the trained weight. \n",
    "\n",
    "- **X-axis**: Different weights of the interaction term\n",
    "- **Y-axis**: Corresponding cost\n",
    "- **Optimal point**: The weight of the interaction term is 6.167, which is the trained weight. The cost is the lowest at this point. \n",
    "\n",
    "The interaction term is important in the model for the non-linear relationship between the stellar mass and temperature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697ae4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_base = w_trained.copy()\n",
    "b_base = b_trained\n",
    "\n",
    "w_mt_values = np.linspace(w_base[3] - 1, w_base[3] + 1, 100)\n",
    "costs = []\n",
    "\n",
    "for val in w_mt_values:\n",
    "    w_temp = w_base.copy()\n",
    "    w_temp[3] = val\n",
    "    costs.append(compute_cost(X_norm, y, w_temp, b_base))\n",
    "\n",
    "plt.plot(w_mt_values, costs)\n",
    "plt.xlabel(\"Interaction Weight w_MT\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost vs Interaction Coefficient\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec71d96",
   "metadata": {},
   "source": [
    "## 8. Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a60d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_new = 1.3\n",
    "T_new = 6600\n",
    "\n",
    "X_new = np.array([M_new, T_new, M_new**2, M_new*T_new])\n",
    "X_new_norm = (X_new - mu) / sigma\n",
    "\n",
    "L_pred_new = predict(X_new_norm, w_trained, b_trained)\n",
    "\n",
    "print(\"Predicted Luminosity for new star:\", L_pred_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "sc = plt.scatter(M, L, c=T, cmap='plasma', s=80, label=\"Observed Stars\")\n",
    "\n",
    "plt.scatter(M_new, L_pred_new, \n",
    "            color='cyan', edgecolor='black', \n",
    "            s=200, marker='*', label=\"New Star Prediction\")\n",
    "\n",
    "plt.xlabel(\"Stellar Mass (M)\")\n",
    "plt.ylabel(\"Luminosity (L)\")\n",
    "plt.title(\"Luminosity Prediction with Temperature Encoding\")\n",
    "\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label(\"Temperature (K)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f26246",
   "metadata": {},
   "source": [
    "## Reasonableness Analysis\n",
    "\n",
    "For a star with M=1.3 and T=6600 K:\n",
    "\n",
    "**Data context:**\n",
    "- Mass: Between 1.2 and 1.4 in our dataset\n",
    "- Temperature: Between 6400 K and 6900 K in our dataset\n",
    "- Expected L range: Should interpolate between 2.30 and 4.10 L☉\n",
    "\n",
    "**Model prediction:** L ≈ 2.73 \n",
    "\n",
    "The prediction falls within the expected range, confirming the model generalizes reasonably for interpolation within the training domain. This is consistent with the empirical mass-luminosity relation for main-sequence stars, where a star slightly more massive than our data points (M=1.3) yields luminosity appropriately positioned in the observed pattern.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
